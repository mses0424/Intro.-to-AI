{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11996156,"sourceType":"datasetVersion","datasetId":7545829}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport os\nimport timm\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Parameters\nimg_size = 64\nbatch_size = 64\nepochs = 30\nlearning_rate = 1e-4\nmin_lr = 1e-6\nweight_decay = 1e-4\npatience = 3\n\n# Paths\ndata_dir = '/kaggle/input/emotion'\nsave_path = '/kaggle/working/emotion_mv_improved.pth'\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# Compute dataset mean and std\ndef compute_mean_std(dataset):\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    mean = 0.\n    std = 0.\n    nb_samples = 0.\n    \n    for data, _ in loader:\n        batch_samples = data.size(0)\n        data = data.view(batch_samples, data.size(1), -1)\n        mean += data.mean(2).sum(0)\n        std += data.std(2).sum(0)\n        nb_samples += batch_samples\n    \n    mean /= nb_samples\n    std /= nb_samples\n    return mean.item(), std.item()\n\ntmp_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n])\n\ntmp_ds = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=tmp_tf)\ngray_mean, gray_std = compute_mean_std(tmp_ds)\nprint(f'Computed gray mean: {gray_mean:.4f}, std: {gray_std:.4f}')\n\n# Data Augmentation\ntrain_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n])\n\ntest_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n])\n\n# Datasets\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_tf)\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_tf)\n\n# Get class information\nclasses = train_dataset.classes\nnum_classes = len(classes)\nprint('Classes:', classes, \"Number of classes:\", num_classes)\n\n# Handle class imbalance\nclass_counts = np.bincount([label for _, label in train_dataset])\nprint(\"Class counts:\", class_counts)\nclass_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\nsample_weights = class_weights[[label for _, label in train_dataset]]\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Data Loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    sampler=sampler,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# CORRECTED MODEL CREATION - Simplified approach\nmodel = timm.create_model(\n    'mobilevit_s',\n    pretrained=True,\n    num_classes=num_classes,  # Set to your actual number of classes\n    img_size=(img_size, img_size),\n    in_chans=1,\n    drop_rate=0.2,\n    drop_path_rate=0.1\n).to(device)\n\n# Print model to verify output size\nprint(model)\nprint(f\"Model output size: {model.num_classes} (should match {num_classes})\")\n\n# Loss function with class weights\nclass_weights = class_weights.to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Optimizer\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=weight_decay\n)\n\n# Scheduler\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='max',\n    factor=0.5,\n    patience=patience,\n    min_lr=min_lr\n)\n\n# Training\nbest_acc = 0.0\nbest_epoch = 0\nearly_stop_counter = 0\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    \n    for imgs, labels in train_loader:\n        # Move data to device\n        imgs, labels = imgs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n    \n    epoch_loss = running_loss / len(train_loader.dataset)\n    \n    # Validation\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            # Move data to device\n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * imgs.size(0)\n            \n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    val_loss /= len(test_loader.dataset)\n    test_acc = correct / total * 100\n    \n    # Update learning rate\n    scheduler.step(test_acc)\n    \n    print(f\"Epoch {epoch:02d}:\")\n    print(f\"  Train Loss: {epoch_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    print(f\"  Val Acc: {test_acc:.2f}%\")\n    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n    \n    if epoch == epochs or test_acc > best_acc:\n        print(\"\\nClassification Report:\")\n        print(classification_report(all_labels, all_preds, target_names=classes))\n    \n    if test_acc > best_acc:\n        best_acc = test_acc\n        best_epoch = epoch\n        early_stop_counter = 0\n        torch.save(model.state_dict(), save_path)\n        print(f\"  New best model saved with acc={best_acc:.2f}%\")\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience * 2:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n    \n    print(\"-\" * 50)\n\nprint(f\"\\nBest Test Accuracy: {best_acc:.2f}% at epoch {best_epoch}\")\n\n# Load best model\nmodel.load_state_dict(torch.load(save_path, map_location=device))\nmodel.eval()\n\n# Prediction function\ndef predict(img_path, return_confidence=False):\n    img = Image.open(img_path).convert('L')\n    x = test_tf(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = model(x)\n        probs = torch.softmax(logits, dim=1)\n    idx = logits.argmax(dim=1).item()\n    if return_confidence:\n        return classes[idx], probs[0, idx].item()\n    return classes[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport os\nimport timm\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Parameters\nimg_size = 96\nbatch_size = 128\nepochs = 100\nlearning_rate = 5e-4\nmin_lr = 1e-6\nweight_decay = 1e-4\npatience = 10\nwarmup_epochs = 5\nnum_workers = 4\n\n# Paths\ndata_dir = '/kaggle/input/emotion'\nsave_path = '/kaggle/working/mobilevit_emotion_best.pth'\nhistory_path = '/kaggle/working/mobilevit_training_history.png'\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# Compute dataset mean and std\ndef compute_mean_std(dataset):\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    mean = 0.\n    std = 0.\n    nb_samples = 0.\n    \n    for data, _ in tqdm(loader, desc=\"Computing mean/std\"):\n        batch_samples = data.size(0)\n        data = data.view(batch_samples, data.size(1), -1)\n        mean += data.mean(2).sum(0)\n        std += data.std(2).sum(0)\n        nb_samples += batch_samples\n    \n    mean /= nb_samples\n    std /= nb_samples\n    return mean.item(), std.item()\n\n# Create temporary transforms\ntmp_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n])\n\nprint(\"Loading dataset for mean/std calculation...\")\ntmp_ds = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=tmp_tf)\ngray_mean, gray_std = compute_mean_std(tmp_ds)\nprint(f'Computed gray mean: {gray_mean:.4f}, std: {gray_std:.4f}')\n\n# Enhanced Data Augmentation\ntrain_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0), ratio=(0.8, 1.2)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 1.0)),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n])\n\ntest_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n])\n\n# Datasets\nprint(\"Creating datasets...\")\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_tf)\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_tf)\n\n# Get class information\nclasses = train_dataset.classes\nnum_classes = len(classes)\nprint('Classes:', classes, \"Number of classes:\", num_classes)\n\n# Handle class imbalance\nclass_counts = np.bincount([label for _, label in train_dataset])\nprint(\"Class counts:\", class_counts)\nclass_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\nsample_weights = class_weights[[label for _, label in train_dataset]]\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Data Loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    sampler=sampler,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True\n)\n\n# Enhanced MobileViT Model with automatic channel detection\nclass MobileViTEmotion(nn.Module):\n    def __init__(self, num_classes, img_size=96):\n        super().__init__()\n        # Use MobileViT as backbone\n        self.backbone = timm.create_model(\n            'mobilevit_s',\n            pretrained=True,\n            features_only=True,\n            in_chans=1\n        )\n        \n        # Get actual channel dimensions from backbone\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 1, img_size, img_size)\n            features = self.backbone(dummy_input)\n            self.last_channel = features[-1].shape[1]\n            self.feature_size = features[-1].shape[2]\n        \n        print(f\"MobileViT feature dimensions: {self.last_channel} channels, {self.feature_size}x{self.feature_size}\")\n        \n        # Feature refinement\n        self.conv1 = nn.Conv2d(self.last_channel, 256, kernel_size=1)\n        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=1),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n            nn.Conv2d(64, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(64, 256),\n            nn.BatchNorm1d(256),\n            nn.SiLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        # Get features from MobileViT\n        features = self.backbone(x)\n        x = features[-1]  # Use the deepest feature map\n        \n        # Feature refinement\n        x = F.silu(self.conv1(x))\n        x = F.silu(self.bn1(self.conv2(x)))\n        x = F.silu(self.bn2(self.conv3(x)))\n        \n        # Attention mechanism\n        att = self.attention(x)\n        x = x * att\n        \n        return self.classifier(x)\n\nmodel = MobileViTEmotion(num_classes=num_classes, img_size=img_size).to(device)\nprint(model)\n\n# Advanced Loss Function\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n        \n        if self.alpha is not None:\n            alpha_t = self.alpha[targets]\n            focal_loss = alpha_t * focal_loss\n            \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        return focal_loss\n\n# Apply class weights\nclass_weights = class_weights.to(device)\ncriterion = FocalLoss(alpha=class_weights, gamma=1.5)\n\n# Optimizer with differential learning rates\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if 'backbone' in name:\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\noptimizer = optim.AdamW(\n    [\n        {'params': backbone_params, 'lr': learning_rate/10},\n        {'params': head_params, 'lr': learning_rate}\n    ],\n    weight_decay=weight_decay\n)\n\n# Combined learning rate schedulers\nscheduler_cosine = CosineAnnealingWarmRestarts(\n    optimizer,\n    T_0=10,\n    T_mult=2,\n    eta_min=min_lr\n)\nscheduler_plateau = ReduceLROnPlateau(\n    optimizer,\n    mode='max',\n    factor=0.5,\n    patience=3\n)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_acc': [],\n    'lr': []\n}\n\n# Training function\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(loader, desc=\"Training\")\n    \n    for imgs, labels in progress:\n        imgs, labels = imgs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n        \n        progress.set_postfix(loss=loss.item())\n    \n    return running_loss / len(loader.dataset)\n\n# Validation function\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        progress = tqdm(loader, desc=\"Validation\")\n        for imgs, labels in progress:\n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * imgs.size(0)\n            \n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = correct / total * 100\n    return running_loss / len(loader.dataset), accuracy, all_preds, all_labels\n\n# Training loop\nbest_acc = 0.0\nearly_stop_counter = 0\n\nprint(\"Starting training...\")\nfor epoch in range(1, epochs + 1):\n    print(f\"\\nEpoch {epoch}/{epochs}\")\n    \n    # Learning rate warmup\n    if epoch <= warmup_epochs:\n        lr_scale = min(1., float(epoch) / warmup_epochs)\n        for pg in optimizer.param_groups:\n            pg['lr'] = lr_scale * pg.get('initial_lr', learning_rate)\n    \n    # Train\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    history['train_loss'].append(train_loss)\n    \n    # Validate\n    val_loss, val_acc, val_preds, val_labels = validate(model, test_loader, criterion, device)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n    \n    # Update learning rate\n    if epoch > warmup_epochs:\n        scheduler_cosine.step()\n        scheduler_plateau.step(val_acc)\n    \n    # Save best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        early_stop_counter = 0\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with accuracy: {val_acc:.2f}%\")\n        \n        # Print classification report for best model\n        print(\"\\nClassification Report:\")\n        print(classification_report(val_labels, val_preds, target_names=classes))\n    else:\n        early_stop_counter += 1\n        print(f\"No improvement for {early_stop_counter}/{patience} epochs\")\n    \n    # Early stopping\n    if early_stop_counter >= patience:\n        print(f\"Early stopping at epoch {epoch}\")\n        break\n\nprint(f\"\\nBest Validation Accuracy: {best_acc:.2f}%\")\n\n# Plot training history\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 1, 1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Val Loss')\nplt.title('Loss History')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(history['val_acc'], 'g-', label='Val Accuracy')\nplt.title('Accuracy History')\nplt.xlabel('Epochs')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(history_path)\nplt.show()\n\n# Load best model\nmodel.load_state_dict(torch.load(save_path, map_location=device))\nmodel.eval()\n\n# Prediction function\ndef predict(img_path, return_confidence=False):\n    img = Image.open(img_path).convert('L')\n    x = test_tf(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = model(x)\n        probs = F.softmax(logits, dim=1)\n    idx = logits.argmax(dim=1).item()\n    confidence = probs[0, idx].item()\n    \n    if return_confidence:\n        return classes[idx], confidence\n    return classes[idx]\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport os\nimport timm\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Enhanced Parameters\nimg_size = 112  # Increased resolution for facial details\nbatch_size = 64  # Reduced for better gradient updates\nepochs = 100  # More training time\nlearning_rate = 1e-3  # Higher initial learning rate\nmin_lr = 1e-6\nweight_decay = 1e-5  # Reduced regularization\npatience = 15\nwarmup_epochs = 10\nnum_workers = 4\nmixup_alpha = 0.4  # Data augmentation\n\n# Paths\ndata_dir = '/kaggle/input/emotion'\nsave_path = '/kaggle/working/mobilevit_emotion_best.pth'\nhistory_path = '/kaggle/working/training_history.png'\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# Compute dataset mean and std\ndef compute_mean_std(dataset):\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    mean = 0.\n    std = 0.\n    nb_samples = 0.\n    \n    for data, _ in tqdm(loader, desc=\"Computing mean/std\"):\n        batch_samples = data.size(0)\n        data = data.view(batch_samples, data.size(1), -1)\n        mean += data.mean(2).sum(0)\n        std += data.std(2).sum(0)\n        nb_samples += batch_samples\n    \n    mean /= nb_samples\n    std /= nb_samples\n    return mean.item(), std.item()\n\n# Create temporary transforms\ntmp_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n])\n\nprint(\"Loading dataset for mean/std calculation...\")\ntmp_ds = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=tmp_tf)\ngray_mean, gray_std = compute_mean_std(tmp_ds)\nprint(f'Computed gray mean: {gray_mean:.4f}, std: {gray_std:.4f}')\n\n# Enhanced Data Augmentation\ntrain_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.4, contrast=0.4),\n    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.7),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n])\n\ntest_tf = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([gray_mean], [gray_std]),\n])\n\n# Datasets\nprint(\"Creating datasets...\")\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_tf)\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_tf)\n\n# Get class information\nclasses = train_dataset.classes\nnum_classes = len(classes)\nprint('Classes:', classes, \"Number of classes:\", num_classes)\n\n# Handle class imbalance\nclass_counts = np.bincount([label for _, label in train_dataset])\nprint(\"Class counts:\", class_counts)\nmax_count = max(class_counts)\nclass_weights = torch.tensor([max_count / count for count in class_counts], dtype=torch.float)\nsample_weights = class_weights[[label for _, label in train_dataset]]\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Data Loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    sampler=sampler,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True\n)\n\n# Enhanced MobileViT Model with Residual Connections\nclass MobileViTEmotion(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Use MobileViT as backbone\n        self.backbone = timm.create_model(\n            'mobilevit_s',\n            pretrained=True,\n            features_only=True,\n            in_chans=1\n        )\n        \n        # Get feature dimensions\n        dummy = torch.zeros(1, 1, img_size, img_size)\n        features = self.backbone(dummy)\n        self.feature_channels = [f.shape[1] for f in features]\n        print(\"Feature channels:\", self.feature_channels)\n        \n        # Feature refinement with residual connections\n        self.conv1 = nn.Conv2d(self.feature_channels[-1], 256, kernel_size=1)\n        self.bn1 = nn.BatchNorm2d(256)\n        \n        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        self.shortcut = nn.Sequential(\n            nn.Conv2d(self.feature_channels[-1], 128, kernel_size=1),\n            nn.BatchNorm2d(128)\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=1),\n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n            nn.Conv2d(64, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n        # Classifier with more capacity\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(128, 512),\n            nn.BatchNorm1d(512),\n            nn.SiLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        # Get features from MobileViT\n        features = self.backbone(x)\n        x = features[-1]  # Use the deepest feature map\n        \n        # Initial transformation\n        x1 = F.silu(self.bn1(self.conv1(x)))\n        \n        # Residual block\n        identity = self.shortcut(x)\n        x2 = F.silu(self.bn2(self.conv2(x1)))\n        x3 = self.bn3(self.conv3(x2))\n        x3 += identity  # Residual connection\n        x = F.silu(x3)\n        \n        # Attention mechanism\n        att = self.attention(x)\n        x = x * att\n        \n        return self.classifier(x)\n\nmodel = MobileViTEmotion(num_classes=num_classes).to(device)\nprint(model)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n\n# Advanced Loss Function with Label Smoothing\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, smoothing=0.1, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.smoothing = smoothing\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        log_probs = F.log_softmax(inputs, dim=-1)\n        nll_loss = -log_probs.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n        \n        # Label smoothing\n        smooth_loss = -log_probs.mean(dim=-1)\n        nll_loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n        \n        # Focal loss\n        pt = torch.exp(-nll_loss)\n        focal_loss = ((1 - pt) ** self.gamma) * nll_loss\n        \n        if self.alpha is not None:\n            alpha_t = self.alpha[targets]\n            focal_loss = alpha_t * focal_loss\n            \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        return focal_loss\n\n# Apply class weights\nclass_weights = class_weights.to(device)\ncriterion = FocalLoss(alpha=class_weights, gamma=2.0, smoothing=0.1)\n\n# Optimizer\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=weight_decay\n)\n\n# Learning rate schedulers\nscheduler_cosine = CosineAnnealingLR(\n    optimizer,\n    T_max=epochs - warmup_epochs,\n    eta_min=min_lr\n)\nscheduler_plateau = ReduceLROnPlateau(\n    optimizer,\n    mode='max',\n    factor=0.5,\n    patience=5\n)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_acc': [],\n    'lr': []\n}\n\n# Mixup augmentation\ndef mixup_data(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n        \n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    \n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n# Training function with mixup\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(loader, desc=\"Training\")\n    \n    for imgs, labels in progress:\n        imgs, labels = imgs.to(device), labels.to(device)\n        \n        # Apply mixup\n        inputs, targets_a, targets_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n        \n        progress.set_postfix(loss=loss.item())\n    \n    return running_loss / len(loader.dataset)\n\n# Validation function\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        progress = tqdm(loader, desc=\"Validation\")\n        for imgs, labels in progress:\n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * imgs.size(0)\n            \n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = correct / total * 100\n    return running_loss / len(loader.dataset), accuracy, all_preds, all_labels\n\n# Training loop\nbest_acc = 0.0\nearly_stop_counter = 0\n\nprint(\"Starting training...\")\nfor epoch in range(1, epochs + 1):\n    print(f\"\\nEpoch {epoch}/{epochs}\")\n    \n    # Learning rate warmup\n    if epoch <= warmup_epochs:\n        lr = learning_rate * (epoch / warmup_epochs)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    \n    # Train\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    history['train_loss'].append(train_loss)\n    \n    # Validate\n    val_loss, val_acc, val_preds, val_labels = validate(model, test_loader, criterion, device)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n    \n    # Update learning rate\n    if epoch > warmup_epochs:\n        scheduler_cosine.step()\n        scheduler_plateau.step(val_acc)\n    \n    # Save best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        early_stop_counter = 0\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with accuracy: {val_acc:.2f}%\")\n        \n        # Print classification report for best model\n        print(\"\\nClassification Report:\")\n        print(classification_report(val_labels, val_preds, target_names=classes))\n    else:\n        early_stop_counter += 1\n        print(f\"No improvement for {early_stop_counter}/{patience} epochs\")\n    \n    # Early stopping\n    if early_stop_counter >= patience:\n        print(f\"Early stopping at epoch {epoch}\")\n        break\n\nprint(f\"\\nBest Validation Accuracy: {best_acc:.2f}%\")\n\n# Plot training history\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 1, 1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Val Loss')\nplt.title('Loss History')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(history['val_acc'], 'g-', label='Val Accuracy')\nplt.title('Accuracy History')\nplt.xlabel('Epochs')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(history_path)\nplt.show()\n\n# Load best model\nmodel.load_state_dict(torch.load(save_path, map_location=device))\nmodel.eval()\n\n# Enhanced prediction function\ndef predict(img_path, return_confidence=False):\n    img = Image.open(img_path).convert('L')\n    x = test_tf(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = model(x)\n        probs = F.softmax(logits, dim=1)\n    idx = logits.argmax(dim=1).item()\n    confidence = probs[0, idx].item()\n    \n    if return_confidence:\n        return classes[idx], confidence\n    return classes[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T04:21:57.784068Z","iopub.execute_input":"2025-05-30T04:21:57.784616Z","execution_failed":"2025-05-30T07:22:07.500Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset for mean/std calculation...\n","output_type":"stream"},{"name":"stderr","text":"Computing mean/std: 100%|██████████| 449/449 [00:31<00:00, 14.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Computed gray mean: 0.5077, std: 0.2063\nCreating datasets...\nClasses: ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised'] Number of classes: 7\nClass counts: [3995  436 4097 7215 4965 4830 3171]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1ab1505dbc4241a00c93d68a989633"}},"metadata":{}},{"name":"stdout","text":"Feature channels: [32, 64, 96, 128, 640]\nMobileViTEmotion(\n  (backbone): FeatureListNet(\n    (stem): ConvNormAct(\n      (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNormAct2d(\n        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n        (drop): Identity()\n        (act): SiLU(inplace=True)\n      )\n    )\n    (stages_0): Sequential(\n      (0): BottleneckBlock(\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n          (bn): BatchNormAct2d(\n            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n    )\n    (stages_1): Sequential(\n      (0): BottleneckBlock(\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n      (1): BottleneckBlock(\n        (shortcut): Identity()\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n      (2): BottleneckBlock(\n        (shortcut): Identity()\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n    )\n    (stages_2): Sequential(\n      (0): BottleneckBlock(\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n          (bn): BatchNormAct2d(\n            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n      (1): MobileVitBlock(\n        (conv_kxk): ConvNormAct(\n          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_1x1): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (transformer): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=144, out_features=432, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=144, out_features=144, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=144, out_features=288, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=288, out_features=144, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (1): Block(\n            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=144, out_features=432, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=144, out_features=144, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=144, out_features=288, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=288, out_features=144, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n        )\n        (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n        (conv_proj): ConvNormAct(\n          (conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_fusion): ConvNormAct(\n          (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (stages_3): Sequential(\n      (0): BottleneckBlock(\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n          (bn): BatchNormAct2d(\n            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n      (1): MobileVitBlock(\n        (conv_kxk): ConvNormAct(\n          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_1x1): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (transformer): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=384, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=384, out_features=192, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (1): Block(\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=384, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=384, out_features=192, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (2): Block(\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=384, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=384, out_features=192, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (3): Block(\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=384, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=384, out_features=192, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n        )\n        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (conv_proj): ConvNormAct(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_fusion): ConvNormAct(\n          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (stages_4): Sequential(\n      (0): BottleneckBlock(\n        (conv1_1x1): ConvNormAct(\n          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2_kxk): ConvNormAct(\n          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n          (bn): BatchNormAct2d(\n            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv2b_kxk): Identity()\n        (attn): Identity()\n        (conv3_1x1): ConvNormAct(\n          (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n        )\n        (attn_last): Identity()\n        (drop_path): Identity()\n        (act): Identity()\n      )\n      (1): MobileVitBlock(\n        (conv_kxk): ConvNormAct(\n          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_1x1): Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (transformer): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=240, out_features=720, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=240, out_features=240, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=240, out_features=480, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=480, out_features=240, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (1): Block(\n            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=240, out_features=720, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=240, out_features=240, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=240, out_features=480, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=480, out_features=240, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n          (2): Block(\n            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=240, out_features=720, bias=True)\n              (q_norm): Identity()\n              (k_norm): Identity()\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=240, out_features=240, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (ls1): Identity()\n            (drop_path1): Identity()\n            (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=240, out_features=480, bias=True)\n              (act): SiLU()\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): Linear(in_features=480, out_features=240, bias=True)\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (ls2): Identity()\n            (drop_path2): Identity()\n          )\n        )\n        (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n        (conv_proj): ConvNormAct(\n          (conv): Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n        (conv_fusion): ConvNormAct(\n          (conv): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNormAct2d(\n            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (final_conv): ConvNormAct(\n      (conv): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNormAct2d(\n        640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n        (drop): Identity()\n        (act): SiLU(inplace=True)\n      )\n    )\n  )\n  (conv1): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (shortcut): Sequential(\n    (0): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (attention): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): SiLU()\n    (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    (4): Sigmoid()\n  )\n  (classifier): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=1)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=128, out_features=512, bias=True)\n    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): SiLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=512, out_features=256, bias=True)\n    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): SiLU()\n    (9): Dropout(p=0.3, inplace=False)\n    (10): Linear(in_features=256, out_features=7, bias=True)\n  )\n)\nTotal parameters: 5.84M\nStarting training...\n\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:04<00:00,  6.95it/s, loss=3.07]\nValidation: 100%|██████████| 113/113 [00:08<00:00, 12.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.7766 | Val Loss: 2.4903 | Val Acc: 10.55%\nNew best model saved with accuracy: 10.55%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.13      0.00      0.01       958\n   disgusted       0.02      0.96      0.04       111\n     fearful       0.00      0.00      0.00      1024\n       happy       0.53      0.01      0.03      1774\n     neutral       0.31      0.03      0.06      1233\n         sad       0.38      0.01      0.01      1247\n   surprised       0.35      0.69      0.47       831\n\n    accuracy                           0.11      7178\n   macro avg       0.25      0.24      0.09      7178\nweighted avg       0.31      0.11      0.07      7178\n\n\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:07<00:00,  6.64it/s, loss=2.35]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 20.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.0506 | Val Loss: 2.0094 | Val Acc: 31.07%\nNew best model saved with accuracy: 31.07%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.33      0.04      0.07       958\n   disgusted       0.04      0.90      0.07       111\n     fearful       0.10      0.00      0.01      1024\n       happy       0.67      0.51      0.58      1774\n     neutral       0.48      0.31      0.38      1233\n         sad       0.39      0.07      0.12      1247\n   surprised       0.39      0.86      0.53       831\n\n    accuracy                           0.31      7178\n   macro avg       0.34      0.38      0.25      7178\nweighted avg       0.42      0.31      0.30      7178\n\n\nEpoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=3.15]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8124 | Val Loss: 1.7206 | Val Acc: 45.32%\nNew best model saved with accuracy: 45.32%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.34      0.41      0.37       958\n   disgusted       0.06      0.81      0.11       111\n     fearful       0.37      0.10      0.16      1024\n       happy       0.88      0.61      0.72      1774\n     neutral       0.48      0.57      0.52      1233\n         sad       0.43      0.21      0.28      1247\n   surprised       0.65      0.73      0.69       831\n\n    accuracy                           0.45      7178\n   macro avg       0.46      0.49      0.41      7178\nweighted avg       0.55      0.45      0.47      7178\n\n\nEpoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.69]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6443 | Val Loss: 1.8311 | Val Acc: 39.34%\nNo improvement for 1/15 epochs\n\nEpoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.68]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.5863 | Val Loss: 1.6298 | Val Acc: 46.99%\nNew best model saved with accuracy: 46.99%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.46      0.34      0.39       958\n   disgusted       0.06      0.90      0.12       111\n     fearful       0.35      0.29      0.31      1024\n       happy       0.91      0.64      0.75      1774\n     neutral       0.58      0.45      0.51      1233\n         sad       0.50      0.18      0.26      1247\n   surprised       0.52      0.87      0.65       831\n\n    accuracy                           0.47      7178\n   macro avg       0.48      0.53      0.43      7178\nweighted avg       0.58      0.47      0.49      7178\n\n\nEpoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.01]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.5286 | Val Loss: 1.6000 | Val Acc: 49.00%\nNew best model saved with accuracy: 49.00%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.37      0.55      0.44       958\n   disgusted       0.11      0.80      0.19       111\n     fearful       0.36      0.37      0.36      1024\n       happy       0.95      0.56      0.70      1774\n     neutral       0.54      0.46      0.50      1233\n         sad       0.52      0.22      0.31      1247\n   surprised       0.55      0.84      0.66       831\n\n    accuracy                           0.49      7178\n   macro avg       0.49      0.54      0.45      7178\nweighted avg       0.58      0.49      0.50      7178\n\n\nEpoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.94]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4879 | Val Loss: 1.6090 | Val Acc: 50.57%\nNew best model saved with accuracy: 50.57%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.46      0.45      0.46       958\n   disgusted       0.07      0.90      0.13       111\n     fearful       0.43      0.29      0.35      1024\n       happy       0.91      0.68      0.78      1774\n     neutral       0.58      0.45      0.51      1233\n         sad       0.50      0.30      0.37      1247\n   surprised       0.63      0.80      0.71       831\n\n    accuracy                           0.51      7178\n   macro avg       0.51      0.55      0.47      7178\nweighted avg       0.61      0.51      0.54      7178\n\n\nEpoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.15]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4505 | Val Loss: 1.5243 | Val Acc: 54.75%\nNew best model saved with accuracy: 54.75%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.48      0.51      0.49       958\n   disgusted       0.14      0.87      0.24       111\n     fearful       0.36      0.30      0.33      1024\n       happy       0.93      0.69      0.79      1774\n     neutral       0.52      0.62      0.57      1233\n         sad       0.55      0.28      0.37      1247\n   surprised       0.58      0.85      0.69       831\n\n    accuracy                           0.55      7178\n   macro avg       0.51      0.59      0.50      7178\nweighted avg       0.60      0.55      0.55      7178\n\n\nEpoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.67]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4636 | Val Loss: 1.8072 | Val Acc: 45.07%\nNo improvement for 1/15 epochs\n\nEpoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.59]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4080 | Val Loss: 1.5505 | Val Acc: 53.48%\nNo improvement for 2/15 epochs\n\nEpoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.36]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.3681 | Val Loss: 1.5079 | Val Acc: 54.47%\nNo improvement for 3/15 epochs\n\nEpoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.63]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.3511 | Val Loss: 1.5109 | Val Acc: 52.28%\nNo improvement for 4/15 epochs\n\nEpoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=3.58]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.3172 | Val Loss: 1.5618 | Val Acc: 44.83%\nNo improvement for 5/15 epochs\n\nEpoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=3.75]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.2857 | Val Loss: 1.4175 | Val Acc: 57.29%\nNew best model saved with accuracy: 57.29%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.52      0.53      0.52       958\n   disgusted       0.21      0.77      0.32       111\n     fearful       0.47      0.25      0.33      1024\n       happy       0.90      0.72      0.80      1774\n     neutral       0.53      0.63      0.57      1233\n         sad       0.54      0.37      0.44      1247\n   surprised       0.50      0.90      0.64       831\n\n    accuracy                           0.57      7178\n   macro avg       0.52      0.60      0.52      7178\nweighted avg       0.60      0.57      0.57      7178\n\n\nEpoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=2.67]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.2404 | Val Loss: 1.3876 | Val Acc: 59.56%\nNew best model saved with accuracy: 59.56%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.56      0.51      0.53       958\n   disgusted       0.19      0.86      0.31       111\n     fearful       0.57      0.23      0.33      1024\n       happy       0.90      0.77      0.83      1774\n     neutral       0.55      0.60      0.57      1233\n         sad       0.48      0.51      0.49      1247\n   surprised       0.60      0.85      0.70       831\n\n    accuracy                           0.60      7178\n   macro avg       0.55      0.62      0.54      7178\nweighted avg       0.63      0.60      0.59      7178\n\n\nEpoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.49]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.2479 | Val Loss: 1.4211 | Val Acc: 59.03%\nNo improvement for 1/15 epochs\n\nEpoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.15]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1797 | Val Loss: 1.5062 | Val Acc: 53.23%\nNo improvement for 2/15 epochs\n\nEpoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.3] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1917 | Val Loss: 1.3976 | Val Acc: 58.76%\nNo improvement for 3/15 epochs\n\nEpoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.81]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1798 | Val Loss: 1.4014 | Val Acc: 59.47%\nNo improvement for 4/15 epochs\n\nEpoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=2.63]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1151 | Val Loss: 1.4645 | Val Acc: 57.31%\nNo improvement for 5/15 epochs\n\nEpoch 21/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=3.48] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1208 | Val Loss: 1.3649 | Val Acc: 60.02%\nNew best model saved with accuracy: 60.02%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.56      0.50      0.53       958\n   disgusted       0.18      0.86      0.30       111\n     fearful       0.47      0.40      0.43      1024\n       happy       0.93      0.75      0.83      1774\n     neutral       0.52      0.70      0.60      1233\n         sad       0.60      0.32      0.42      1247\n   surprised       0.62      0.86      0.72       831\n\n    accuracy                           0.60      7178\n   macro avg       0.55      0.63      0.55      7178\nweighted avg       0.64      0.60      0.60      7178\n\n\nEpoch 22/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.34] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1548 | Val Loss: 1.4184 | Val Acc: 61.62%\nNew best model saved with accuracy: 61.62%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.54      0.59      0.56       958\n   disgusted       0.18      0.85      0.29       111\n     fearful       0.58      0.28      0.38      1024\n       happy       0.92      0.77      0.84      1774\n     neutral       0.60      0.59      0.59      1233\n         sad       0.49      0.59      0.54      1247\n   surprised       0.72      0.78      0.75       831\n\n    accuracy                           0.62      7178\n   macro avg       0.58      0.64      0.56      7178\nweighted avg       0.66      0.62      0.62      7178\n\n\nEpoch 23/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=2.5]  \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1225 | Val Loss: 1.4609 | Val Acc: 56.94%\nNo improvement for 1/15 epochs\n\nEpoch 24/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.37it/s, loss=1.71]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0840 | Val Loss: 1.4023 | Val Acc: 60.25%\nNo improvement for 2/15 epochs\n\nEpoch 25/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.39] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0982 | Val Loss: 1.3743 | Val Acc: 60.28%\nNo improvement for 3/15 epochs\n\nEpoch 26/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.46]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1047 | Val Loss: 1.3910 | Val Acc: 60.88%\nNo improvement for 4/15 epochs\n\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.19] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9966 | Val Loss: 1.3562 | Val Acc: 62.40%\nNew best model saved with accuracy: 62.40%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.62      0.48      0.54       958\n   disgusted       0.26      0.76      0.39       111\n     fearful       0.55      0.33      0.41      1024\n       happy       0.90      0.80      0.85      1774\n     neutral       0.54      0.71      0.61      1233\n         sad       0.50      0.51      0.51      1247\n   surprised       0.66      0.79      0.72       831\n\n    accuracy                           0.62      7178\n   macro avg       0.57      0.63      0.58      7178\nweighted avg       0.64      0.62      0.62      7178\n\n\nEpoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=3.05] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1405 | Val Loss: 1.4239 | Val Acc: 61.12%\nNo improvement for 1/15 epochs\n\nEpoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.7]  \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0457 | Val Loss: 1.3745 | Val Acc: 59.84%\nNo improvement for 2/15 epochs\n\nEpoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.87]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9786 | Val Loss: 1.3602 | Val Acc: 62.86%\nNew best model saved with accuracy: 62.86%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.49      0.65      0.56       958\n   disgusted       0.22      0.75      0.34       111\n     fearful       0.51      0.36      0.42      1024\n       happy       0.92      0.79      0.85      1774\n     neutral       0.60      0.64      0.62      1233\n         sad       0.58      0.45      0.51      1247\n   surprised       0.67      0.83      0.74       831\n\n    accuracy                           0.63      7178\n   macro avg       0.57      0.64      0.58      7178\nweighted avg       0.65      0.63      0.63      7178\n\n\nEpoch 31/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.27] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0317 | Val Loss: 1.3340 | Val Acc: 64.53%\nNew best model saved with accuracy: 64.53%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.58      0.59      0.58       958\n   disgusted       0.26      0.86      0.40       111\n     fearful       0.53      0.36      0.43      1024\n       happy       0.86      0.85      0.86      1774\n     neutral       0.62      0.63      0.63      1233\n         sad       0.53      0.52      0.53      1247\n   surprised       0.71      0.81      0.76       831\n\n    accuracy                           0.65      7178\n   macro avg       0.59      0.66      0.60      7178\nweighted avg       0.65      0.65      0.64      7178\n\n\nEpoch 32/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.59] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9889 | Val Loss: 1.3321 | Val Acc: 61.76%\nNo improvement for 1/15 epochs\n\nEpoch 33/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=3.28] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9611 | Val Loss: 1.4061 | Val Acc: 59.06%\nNo improvement for 2/15 epochs\n\nEpoch 34/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=1.96] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9874 | Val Loss: 1.3708 | Val Acc: 62.32%\nNo improvement for 3/15 epochs\n\nEpoch 35/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.37it/s, loss=2.77] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9842 | Val Loss: 1.3190 | Val Acc: 64.50%\nNo improvement for 4/15 epochs\n\nEpoch 36/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.32] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0046 | Val Loss: 1.3033 | Val Acc: 64.27%\nNo improvement for 5/15 epochs\n\nEpoch 37/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=1.64] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0030 | Val Loss: 1.3347 | Val Acc: 63.47%\nNo improvement for 6/15 epochs\n\nEpoch 38/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.89] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9007 | Val Loss: 1.2434 | Val Acc: 65.55%\nNew best model saved with accuracy: 65.55%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.55      0.66      0.60       958\n   disgusted       0.38      0.81      0.52       111\n     fearful       0.55      0.37      0.44      1024\n       happy       0.93      0.80      0.86      1774\n     neutral       0.56      0.73      0.63      1233\n         sad       0.59      0.47      0.52      1247\n   surprised       0.71      0.84      0.77       831\n\n    accuracy                           0.66      7178\n   macro avg       0.61      0.67      0.62      7178\nweighted avg       0.67      0.66      0.65      7178\n\n\nEpoch 39/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=2.06] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8922 | Val Loss: 1.2715 | Val Acc: 65.62%\nNew best model saved with accuracy: 65.62%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.59      0.61      0.60       958\n   disgusted       0.35      0.82      0.49       111\n     fearful       0.55      0.40      0.46      1024\n       happy       0.94      0.79      0.86      1774\n     neutral       0.55      0.75      0.64      1233\n         sad       0.59      0.48      0.53      1247\n   surprised       0.70      0.85      0.77       831\n\n    accuracy                           0.66      7178\n   macro avg       0.61      0.67      0.62      7178\nweighted avg       0.67      0.66      0.66      7178\n\n\nEpoch 40/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.43] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9084 | Val Loss: 1.2834 | Val Acc: 65.98%\nNew best model saved with accuracy: 65.98%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.53      0.68      0.60       958\n   disgusted       0.34      0.70      0.46       111\n     fearful       0.58      0.39      0.47      1024\n       happy       0.94      0.79      0.86      1774\n     neutral       0.56      0.75      0.64      1233\n         sad       0.62      0.45      0.52      1247\n   surprised       0.72      0.85      0.78       831\n\n    accuracy                           0.66      7178\n   macro avg       0.61      0.66      0.62      7178\nweighted avg       0.68      0.66      0.66      7178\n\n\nEpoch 41/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.31it/s, loss=2.16] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8103 | Val Loss: 1.2906 | Val Acc: 65.41%\nNo improvement for 1/15 epochs\n\nEpoch 42/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=0.797]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8094 | Val Loss: 1.2794 | Val Acc: 66.06%\nNew best model saved with accuracy: 66.06%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.54      0.67      0.60       958\n   disgusted       0.44      0.74      0.55       111\n     fearful       0.55      0.41      0.47      1024\n       happy       0.93      0.81      0.87      1774\n     neutral       0.56      0.75      0.64      1233\n         sad       0.60      0.44      0.51      1247\n   surprised       0.72      0.84      0.77       831\n\n    accuracy                           0.66      7178\n   macro avg       0.62      0.66      0.63      7178\nweighted avg       0.67      0.66      0.66      7178\n\n\nEpoch 43/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=3.16] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8235 | Val Loss: 1.2507 | Val Acc: 66.68%\nNew best model saved with accuracy: 66.68%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.56      0.63      0.59       958\n   disgusted       0.44      0.77      0.56       111\n     fearful       0.55      0.46      0.50      1024\n       happy       0.92      0.82      0.87      1774\n     neutral       0.59      0.72      0.65      1233\n         sad       0.60      0.46      0.52      1247\n   surprised       0.70      0.85      0.77       831\n\n    accuracy                           0.67      7178\n   macro avg       0.62      0.67      0.64      7178\nweighted avg       0.67      0.67      0.66      7178\n\n\nEpoch 44/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.33] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7982 | Val Loss: 1.2635 | Val Acc: 67.15%\nNew best model saved with accuracy: 67.15%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.56      0.67      0.61       958\n   disgusted       0.42      0.77      0.55       111\n     fearful       0.56      0.43      0.48      1024\n       happy       0.92      0.82      0.87      1774\n     neutral       0.60      0.71      0.65      1233\n         sad       0.59      0.51      0.55      1247\n   surprised       0.74      0.82      0.78       831\n\n    accuracy                           0.67      7178\n   macro avg       0.63      0.68      0.64      7178\nweighted avg       0.68      0.67      0.67      7178\n\n\nEpoch 45/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=2.33] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8780 | Val Loss: 1.2743 | Val Acc: 66.43%\nNo improvement for 1/15 epochs\n\nEpoch 46/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.85] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8351 | Val Loss: 1.2978 | Val Acc: 65.94%\nNo improvement for 2/15 epochs\n\nEpoch 47/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=0.747]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8222 | Val Loss: 1.2303 | Val Acc: 66.94%\nNo improvement for 3/15 epochs\n\nEpoch 48/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.51] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7711 | Val Loss: 1.2683 | Val Acc: 65.87%\nNo improvement for 4/15 epochs\n\nEpoch 49/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.18] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7910 | Val Loss: 1.2986 | Val Acc: 65.37%\nNo improvement for 5/15 epochs\n\nEpoch 50/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.32it/s, loss=1.11] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7950 | Val Loss: 1.2119 | Val Acc: 66.84%\nNo improvement for 6/15 epochs\n\nEpoch 51/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.12] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7498 | Val Loss: 1.2581 | Val Acc: 67.00%\nNo improvement for 7/15 epochs\n\nEpoch 52/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.73] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7379 | Val Loss: 1.2464 | Val Acc: 67.08%\nNo improvement for 8/15 epochs\n\nEpoch 53/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=1.94] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6981 | Val Loss: 1.2375 | Val Acc: 67.68%\nNew best model saved with accuracy: 67.68%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.57      0.67      0.61       958\n   disgusted       0.48      0.74      0.58       111\n     fearful       0.55      0.51      0.53      1024\n       happy       0.93      0.81      0.87      1774\n     neutral       0.59      0.73      0.65      1233\n         sad       0.64      0.45      0.52      1247\n   surprised       0.73      0.86      0.79       831\n\n    accuracy                           0.68      7178\n   macro avg       0.64      0.68      0.65      7178\nweighted avg       0.69      0.68      0.68      7178\n\n\nEpoch 54/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.26] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7338 | Val Loss: 1.1838 | Val Acc: 68.12%\nNew best model saved with accuracy: 68.12%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.56      0.68      0.61       958\n   disgusted       0.73      0.72      0.73       111\n     fearful       0.56      0.48      0.52      1024\n       happy       0.92      0.84      0.88      1774\n     neutral       0.59      0.73      0.65      1233\n         sad       0.62      0.46      0.53      1247\n   surprised       0.73      0.85      0.79       831\n\n    accuracy                           0.68      7178\n   macro avg       0.67      0.68      0.67      7178\nweighted avg       0.69      0.68      0.68      7178\n\n\nEpoch 55/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.83] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7149 | Val Loss: 1.2827 | Val Acc: 66.90%\nNo improvement for 1/15 epochs\n\nEpoch 56/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=0.69] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6781 | Val Loss: 1.2484 | Val Acc: 67.99%\nNo improvement for 2/15 epochs\n\nEpoch 57/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.52] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7300 | Val Loss: 1.2567 | Val Acc: 67.72%\nNo improvement for 3/15 epochs\n\nEpoch 58/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=2.73] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7244 | Val Loss: 1.2296 | Val Acc: 68.14%\nNew best model saved with accuracy: 68.14%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.57      0.68      0.62       958\n   disgusted       0.49      0.73      0.59       111\n     fearful       0.59      0.45      0.51      1024\n       happy       0.92      0.83      0.87      1774\n     neutral       0.60      0.73      0.66      1233\n         sad       0.61      0.50      0.55      1247\n   surprised       0.72      0.86      0.78       831\n\n    accuracy                           0.68      7178\n   macro avg       0.64      0.68      0.65      7178\nweighted avg       0.69      0.68      0.68      7178\n\n\nEpoch 59/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.53] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6791 | Val Loss: 1.2237 | Val Acc: 67.69%\nNo improvement for 1/15 epochs\n\nEpoch 60/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.31it/s, loss=2.53] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8016 | Val Loss: 1.2346 | Val Acc: 67.75%\nNo improvement for 2/15 epochs\n\nEpoch 61/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.44] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6960 | Val Loss: 1.2128 | Val Acc: 68.14%\nNo improvement for 3/15 epochs\n\nEpoch 62/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.11] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6374 | Val Loss: 1.1984 | Val Acc: 68.10%\nNo improvement for 4/15 epochs\n\nEpoch 63/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.21] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6972 | Val Loss: 1.2256 | Val Acc: 68.50%\nNew best model saved with accuracy: 68.50%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.57      0.66      0.61       958\n   disgusted       0.44      0.75      0.55       111\n     fearful       0.56      0.51      0.54      1024\n       happy       0.93      0.83      0.88      1774\n     neutral       0.61      0.72      0.66      1233\n         sad       0.62      0.51      0.56      1247\n   surprised       0.77      0.83      0.79       831\n\n    accuracy                           0.69      7178\n   macro avg       0.64      0.69      0.66      7178\nweighted avg       0.69      0.69      0.69      7178\n\n\nEpoch 64/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.47] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6313 | Val Loss: 1.2453 | Val Acc: 66.84%\nNo improvement for 1/15 epochs\n\nEpoch 65/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.1]  \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6515 | Val Loss: 1.1908 | Val Acc: 68.61%\nNew best model saved with accuracy: 68.61%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.58      0.67      0.62       958\n   disgusted       0.67      0.72      0.70       111\n     fearful       0.56      0.48      0.52      1024\n       happy       0.92      0.84      0.88      1774\n     neutral       0.59      0.75      0.66      1233\n         sad       0.62      0.48      0.54      1247\n   surprised       0.77      0.85      0.80       831\n\n    accuracy                           0.69      7178\n   macro avg       0.67      0.68      0.67      7178\nweighted avg       0.69      0.69      0.68      7178\n\n\nEpoch 66/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.78] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6303 | Val Loss: 1.2387 | Val Acc: 67.64%\nNo improvement for 1/15 epochs\n\nEpoch 67/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.01] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7695 | Val Loss: 1.2335 | Val Acc: 68.18%\nNo improvement for 2/15 epochs\n\nEpoch 68/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=0.795]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7240 | Val Loss: 1.2097 | Val Acc: 68.64%\nNew best model saved with accuracy: 68.64%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.60      0.64      0.62       958\n   disgusted       0.57      0.72      0.63       111\n     fearful       0.54      0.54      0.54      1024\n       happy       0.94      0.82      0.87      1774\n     neutral       0.58      0.75      0.66      1233\n         sad       0.63      0.49      0.55      1247\n   surprised       0.76      0.84      0.80       831\n\n    accuracy                           0.69      7178\n   macro avg       0.66      0.68      0.67      7178\nweighted avg       0.70      0.69      0.69      7178\n\n\nEpoch 69/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=1.04] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6844 | Val Loss: 1.2648 | Val Acc: 67.34%\nNo improvement for 1/15 epochs\n\nEpoch 70/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.37it/s, loss=2.69] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7005 | Val Loss: 1.2264 | Val Acc: 68.32%\nNo improvement for 2/15 epochs\n\nEpoch 71/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=0.854]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6338 | Val Loss: 1.2283 | Val Acc: 68.47%\nNo improvement for 3/15 epochs\n\nEpoch 72/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=0.781]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6445 | Val Loss: 1.2463 | Val Acc: 67.99%\nNo improvement for 4/15 epochs\n\nEpoch 73/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=0.855]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6532 | Val Loss: 1.2140 | Val Acc: 68.46%\nNo improvement for 5/15 epochs\n\nEpoch 74/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.32it/s, loss=0.797]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6921 | Val Loss: 1.2206 | Val Acc: 68.67%\nNew best model saved with accuracy: 68.67%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.57      0.68      0.62       958\n   disgusted       0.58      0.72      0.65       111\n     fearful       0.55      0.50      0.53      1024\n       happy       0.93      0.83      0.88      1774\n     neutral       0.61      0.72      0.66      1233\n         sad       0.63      0.49      0.55      1247\n   surprised       0.76      0.85      0.80       831\n\n    accuracy                           0.69      7178\n   macro avg       0.66      0.69      0.67      7178\nweighted avg       0.69      0.69      0.69      7178\n\n\nEpoch 75/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.03] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6165 | Val Loss: 1.2730 | Val Acc: 67.47%\nNo improvement for 1/15 epochs\n\nEpoch 76/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.34it/s, loss=2.21] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6617 | Val Loss: 1.2498 | Val Acc: 68.47%\nNo improvement for 2/15 epochs\n\nEpoch 77/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.09] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7063 | Val Loss: 1.2243 | Val Acc: 68.82%\nNew best model saved with accuracy: 68.82%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.59      0.66      0.62       958\n   disgusted       0.56      0.73      0.64       111\n     fearful       0.58      0.49      0.53      1024\n       happy       0.93      0.84      0.88      1774\n     neutral       0.58      0.75      0.66      1233\n         sad       0.61      0.50      0.55      1247\n   surprised       0.78      0.84      0.81       831\n\n    accuracy                           0.69      7178\n   macro avg       0.66      0.69      0.67      7178\nweighted avg       0.70      0.69      0.69      7178\n\n\nEpoch 78/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=2.29] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6763 | Val Loss: 1.2099 | Val Acc: 68.81%\nNo improvement for 1/15 epochs\n\nEpoch 79/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.36] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6135 | Val Loss: 1.1983 | Val Acc: 68.99%\nNew best model saved with accuracy: 68.99%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.59      0.66      0.62       958\n   disgusted       0.64      0.71      0.67       111\n     fearful       0.56      0.50      0.53      1024\n       happy       0.93      0.85      0.88      1774\n     neutral       0.59      0.74      0.66      1233\n         sad       0.62      0.49      0.55      1247\n   surprised       0.76      0.85      0.80       831\n\n    accuracy                           0.69      7178\n   macro avg       0.67      0.69      0.67      7178\nweighted avg       0.70      0.69      0.69      7178\n\n\nEpoch 80/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=1.97] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6612 | Val Loss: 1.2259 | Val Acc: 68.28%\nNo improvement for 1/15 epochs\n\nEpoch 81/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=1.91] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6818 | Val Loss: 1.2063 | Val Acc: 69.13%\nNew best model saved with accuracy: 69.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       angry       0.58      0.67      0.62       958\n   disgusted       0.60      0.73      0.66       111\n     fearful       0.55      0.52      0.54      1024\n       happy       0.93      0.84      0.88      1774\n     neutral       0.62      0.73      0.67      1233\n         sad       0.63      0.49      0.55      1247\n   surprised       0.75      0.85      0.79       831\n\n    accuracy                           0.69      7178\n   macro avg       0.67      0.69      0.67      7178\nweighted avg       0.70      0.69      0.69      7178\n\n\nEpoch 82/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=3.14] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6674 | Val Loss: 1.2423 | Val Acc: 68.45%\nNo improvement for 1/15 epochs\n\nEpoch 83/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.73] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6057 | Val Loss: 1.2663 | Val Acc: 67.90%\nNo improvement for 2/15 epochs\n\nEpoch 84/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.39] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6088 | Val Loss: 1.2626 | Val Acc: 68.33%\nNo improvement for 3/15 epochs\n\nEpoch 85/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.33it/s, loss=0.742]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6246 | Val Loss: 1.2221 | Val Acc: 68.96%\nNo improvement for 4/15 epochs\n\nEpoch 86/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.37it/s, loss=2.26] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6618 | Val Loss: 1.2637 | Val Acc: 68.18%\nNo improvement for 5/15 epochs\n\nEpoch 87/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.36it/s, loss=0.875]\nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6762 | Val Loss: 1.2621 | Val Acc: 68.06%\nNo improvement for 6/15 epochs\n\nEpoch 88/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:11<00:00,  6.32it/s, loss=2.26] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6591 | Val Loss: 1.3142 | Val Acc: 65.12%\nNo improvement for 7/15 epochs\n\nEpoch 89/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 449/449 [01:10<00:00,  6.35it/s, loss=2.22] \nValidation: 100%|██████████| 113/113 [00:05<00:00, 21.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6843 | Val Loss: 1.2592 | Val Acc: 68.18%\nNo improvement for 8/15 epochs\n\nEpoch 90/100\n","output_type":"stream"},{"name":"stderr","text":"Training:  58%|█████▊    | 262/449 [00:41<00:29,  6.38it/s, loss=2.37] ","output_type":"stream"}],"execution_count":null}]}